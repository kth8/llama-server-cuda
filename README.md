# llama.cpp CUDA Server Docker Image

This repository provides a ready-to-use Docker image containing the `llama.cpp` server, compiled with CUDA 12 support to enable GPU-accelerated inference. Designed for easy deployment on systems equipped with compatible NVIDIA GPUs, this image offers a convenient containerized solution for running the `llama.cpp` HTTP server and serving large language models efficiently.
