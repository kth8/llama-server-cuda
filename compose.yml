services:
  llama-cuda:
    container_name: llama-cuda
    image: ghcr.io/kth8/llama-server-cuda:latest
    init: true
    ports:
      - "8080:8080"
    volumes:
      - llama:/root/.cache
    environment:
      - LLAMA_ARG_HF_REPO=ggml-org/Qwen2.5-VL-3B-Instruct-GGUF
    cap_drop:
      - all
    labels:
      io.containers.autoupdate: registry
    pull_policy: newer
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  llama:
